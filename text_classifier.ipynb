{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12990459,"sourceType":"datasetVersion","datasetId":8222394},{"sourceId":12995018,"sourceType":"datasetVersion","datasetId":8225644},{"sourceId":12998680,"sourceType":"datasetVersion","datasetId":8228185},{"sourceId":569017,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":427691,"modelId":444697}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom transformers import (\n    XLMRobertaTokenizer, \n    XLMRobertaForSequenceClassification, \n    Trainer, \n    TrainingArguments,\n    EarlyStoppingCallback,\n    logging\n)\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset\nfrom tqdm.auto import tqdm\nimport gc\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Set logging\nlogging.set_verbosity_error()\n\nclass AdvancedBinaryClassifier:\n    def __init__(self, model_name='xlm-roberta-base'):\n        \"\"\"Advanced classifier with confidence boosting techniques\"\"\"\n        self.model_name = model_name\n        \n        # GPU setup\n        if torch.cuda.is_available():\n            self.device = torch.device('cuda')\n            print(f\"✅ GPU: {torch.cuda.get_device_name()}\")\n            print(f\"✅ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n            torch.cuda.empty_cache()\n        else:\n            self.device = torch.device('cpu')\n            print(\"❌ Using CPU\")\n        \n        self.tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n        self.df = None\n        self.class_weights = None\n        \n        # Advanced settings\n        torch.backends.cudnn.benchmark = True\n        if torch.cuda.is_available():\n            torch.cuda.set_per_process_memory_fraction(0.9)\n\n    def load_data(self, file_path):\n        try:\n            print(f\"📂 Loading: {file_path}\")\n            \n            if file_path.endswith('.csv'):\n                encodings = ['utf-8', 'utf-8-sig', 'latin1', 'cp1252', 'windows-1256']\n                for encoding in encodings:\n                    try:\n                        self.df = pd.read_csv(file_path, encoding=encoding)\n                        print(f\"✅ Loaded with {encoding}\")\n                        break\n                    except:\n                        continue\n                else:\n                    self.df = pd.read_csv(file_path, encoding='utf-8', errors='replace')\n            elif file_path.endswith(('.xlsx', '.xls')):\n                self.df = pd.read_excel(file_path)\n            \n            # Advanced data cleaning\n            self.df = self.df.dropna()\n            self.df = self.df.drop_duplicates()\n            \n            print(f\"📊 Shape: {self.df.shape}\")\n            print(f\"📂 Columns: {list(self.df.columns)}\")\n            return list(self.df.columns)\n            \n        except Exception as e:\n            print(f\"❌ Error: {e}\")\n            raise\n\n    def prepare_data(self, label_column='label', text_column='text'):\n        try:\n            print(f\"🔄 Preparing data...\")\n            \n            if label_column not in self.df.columns:\n                raise ValueError(f\"Label column '{label_column}' not found\")\n            if text_column not in self.df.columns:\n                raise ValueError(f\"Text column '{text_column}' not found\")\n            \n            # Get texts\n            text_data = self.df[text_column].astype(str).tolist()\n            \n            # Handle labels\n            labels = self.df[label_column].astype(int).tolist()\n            \n            # Ensure same length\n            min_length = min(len(text_data), len(labels))\n            text_data = text_data[:min_length]\n            labels = labels[:min_length]\n            \n            # Calculate class weights for imbalanced datasets\n            unique_labels = np.unique(labels)\n            self.class_weights = compute_class_weight(\n                'balanced', \n                classes=unique_labels, \n                y=labels\n            )\n            self.class_weights = torch.tensor(self.class_weights, dtype=torch.float).to(self.device)\n            \n            label_counts = pd.Series(labels).value_counts().sort_index()\n            print(f\"📊 Label distribution: {dict(label_counts)}\")\n            print(f\"⚖️ Class weights: {self.class_weights.cpu().numpy()}\")\n            \n            return text_data, labels\n            \n        except Exception as e:\n            print(f\"❌ Error preparing data: {e}\")\n            raise\n\n    def create_dataset(self, texts, labels, max_length=384):\n        print(f\"🔄 Creating dataset with {len(texts)} samples...\")\n        \n        class EnhancedTextDataset(Dataset):\n            def __init__(self, texts, labels, tokenizer, max_length):\n                self.texts = texts\n                self.labels = labels\n                self.tokenizer = tokenizer\n                self.max_length = max_length\n            \n            def __len__(self):\n                return len(self.texts)\n            \n            def __getitem__(self, idx):\n                text = str(self.texts[idx])\n                label = int(self.labels[idx])\n                \n                encoding = self.tokenizer(\n                    text,\n                    truncation=True,\n                    padding='max_length',\n                    max_length=self.max_length,\n                    return_tensors='pt',\n                    add_special_tokens=True,\n                    return_attention_mask=True\n                )\n                \n                return {\n                    'input_ids': encoding['input_ids'].flatten(),\n                    'attention_mask': encoding['attention_mask'].flatten(),\n                    'labels': torch.tensor(label, dtype=torch.long)\n                }\n        \n        return EnhancedTextDataset(texts, labels, self.tokenizer, max_length)\n\n    def compute_metrics(self, eval_pred):\n        predictions, labels = eval_pred\n        preds = np.argmax(predictions, axis=1)\n        \n        probs = torch.softmax(torch.tensor(predictions), dim=-1).numpy()\n        max_probs = np.max(probs, axis=1)\n        avg_confidence = np.mean(max_probs)\n        \n        accuracy = accuracy_score(labels, preds)\n        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)\n        \n        tn, fp, fn, tp = confusion_matrix(labels, preds, labels=[0, 1]).ravel()\n        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n        balanced_accuracy = (recall + specificity) / 2\n        \n        return {\n            'accuracy': float(accuracy),\n            'balanced_accuracy': float(balanced_accuracy),\n            'f1': float(f1),\n            'precision': float(precision),\n            'recall': float(recall),\n            'specificity': float(specificity),\n            'avg_confidence': float(avg_confidence),\n            'true_positives': int(tp),\n            'true_negatives': int(tn),\n            'false_positives': int(fp),\n            'false_negatives': int(fn)\n        }\n\n    def train(self, text_train, y_train, text_val, y_val, \n              save_model_path='./advanced_model',\n              num_epochs=5,\n              batch_size=12,\n              learning_rate=1e-5,\n              max_length=384):\n        try:\n            print(f\"🚀 Advanced Training Started...\")\n            print(f\"📊 Training: {len(text_train)} | Validation: {len(text_val)}\")\n            \n            train_dataset = self.create_dataset(text_train, y_train, max_length)\n            val_dataset = self.create_dataset(text_val, y_val, max_length)\n            \n            print(f\"🔄 Loading enhanced model...\")\n            model = XLMRobertaForSequenceClassification.from_pretrained(\n                self.model_name,\n                num_labels=2,\n                problem_type=\"single_label_classification\",\n                hidden_dropout_prob=0.1,\n                attention_probs_dropout_prob=0.1,\n                classifier_dropout=0.2\n            )\n            \n            if self.class_weights is not None:\n                model.loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n            \n            model = model.to(self.device)\n            print(f\"✅ Model on: {next(model.parameters()).device}\")\n            \n            training_args = TrainingArguments(\n                output_dir='./results',\n                num_train_epochs=num_epochs,\n                per_device_train_batch_size=batch_size,\n                per_device_eval_batch_size=batch_size,\n                gradient_accumulation_steps=2,\n                warmup_ratio=0.1,\n                weight_decay=0.01,\n                learning_rate=learning_rate,\n                \n                eval_strategy=\"epoch\",\n                save_strategy=\"epoch\",\n                logging_steps=25,\n                \n                load_best_model_at_end=True,\n                metric_for_best_model=\"f1\",\n                greater_is_better=True,\n                \n                fp16=True,\n                gradient_checkpointing=True,\n                dataloader_pin_memory=False,\n                dataloader_num_workers=0,\n                \n                max_grad_norm=1.0,\n                \n                seed=42,\n                report_to=\"none\",\n                remove_unused_columns=True,\n            )\n            \n            trainer = Trainer(\n                model=model,\n                args=training_args,\n                train_dataset=train_dataset,\n                eval_dataset=val_dataset,\n                compute_metrics=self.compute_metrics,\n                callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n            )\n            \n            print(f\"🚀 Training with advanced techniques...\")\n            trainer.train()\n            \n            print(f\"💾 Saving to {save_model_path}\")\n            trainer.save_model(save_model_path)\n            self.tokenizer.save_pretrained(save_model_path)\n            \n            print(f\"📊 Final evaluation...\")\n            eval_results = trainer.evaluate()\n            \n            print(\"\\n\" + \"=\"*60)\n            print(\"🏆 ADVANCED TRAINING RESULTS:\")\n            for key, value in eval_results.items():\n                if isinstance(value, float):\n                    if 'confidence' in key:\n                        print(f\"{key}: {value:.1%}\")\n                    else:\n                        print(f\"{key}: {value:.4f}\")\n                else:\n                    print(f\"{key}: {value}\")\n            print(\"=\"*60)\n            \n            return eval_results\n            \n        except Exception as e:\n            print(f\"❌ Training error: {e}\")\n            raise\n        finally:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                gc.collect()\n\n    def predict_with_analysis(self, texts, model_path, confidence_threshold=0.9):\n        \n        try:\n            print(f\"🔄 Loading model for enhanced prediction...\")\n            model = XLMRobertaForSequenceClassification.from_pretrained(model_path)\n            tokenizer = XLMRobertaTokenizer.from_pretrained(model_path)\n            \n            model = model.to(self.device)\n            model.eval()\n            \n            results = []\n            all_confidences = []\n            \n            print(f\"🔮 Analyzing {len(texts)} samples...\")\n            \n            for text in tqdm(texts):\n                processed_text = text  # ✅ keep raw text\n                \n                inputs = tokenizer(\n                    processed_text,\n                    return_tensors='pt',\n                    truncation=True,\n                    padding=True,\n                    max_length=384\n                ).to(self.device)\n                \n                with torch.no_grad():\n                    outputs = model(**inputs)\n                    logits = outputs.logits\n                    probs = torch.softmax(logits, dim=-1)\n                    \n                    pred = torch.argmax(logits, dim=-1).cpu().item()\n                    conf = torch.max(probs).cpu().item()\n                    \n                    prob_not_allowed = probs[0][0].cpu().item()\n                    prob_allowed = probs[0][1].cpu().item()\n                    \n                    pred_label = \"not allowed\" if pred == 0 else \"allowed\"\n                    confidence_level = \"HIGH\" if conf >= confidence_threshold else \"MEDIUM\" if conf >= 0.7 else \"LOW\"\n                    \n                    results.append({\n                        'text': text,\n                        'processed_text': processed_text,\n                        'prediction': pred_label,\n                        'confidence': conf,\n                        'confidence_level': confidence_level,\n                        'prob_not_allowed': prob_not_allowed,\n                        'prob_allowed': prob_allowed,\n                        'prediction_numeric': pred\n                    })\n                    \n                    all_confidences.append(conf)\n            \n            avg_confidence = np.mean(all_confidences)\n            high_confidence_count = sum(1 for c in all_confidences if c >= confidence_threshold)\n            \n            print(f\"\\n📊 PREDICTION ANALYSIS:\")\n            print(f\"Average confidence: {avg_confidence:.1%}\")\n            print(f\"High confidence predictions: {high_confidence_count}/{len(texts)} ({high_confidence_count/len(texts):.1%})\")\n            \n            return results\n            \n        except Exception as e:\n            print(f\"❌ Prediction error: {e}\")\n            raise\n        finally:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                \ndef main():\n    print(\"=\"*60)\n    print(\"🚀 ADVANCED BINARY TEXT CLASSIFIER\")\n    print(\"=\"*60)\n    \n    print(f\"PyTorch: {torch.__version__}\")\n    print(f\"CUDA: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        print(f\"GPU: {torch.cuda.get_device_name()}\")\n    \n    classifier = AdvancedBinaryClassifier()\n    data_path = \"/kaggle/input/finalfinal/yarbitslh.csv\"\n    \n    try:\n        classifier.load_data(data_path)\n        texts, labels = classifier.prepare_data()\n        \n        text_train, text_val, y_train, y_val = train_test_split(\n            texts, labels,\n            test_size=0.15,\n            random_state=42,\n            stratify=labels\n        )\n        \n        print(f\"📊 Training: {len(text_train)} | Validation: {len(text_val)}\")\n        \n        results = classifier.train(\n            text_train, y_train, text_val, y_val,\n            save_model_path='./advanced_binary_model',\n            num_epochs=5,\n            batch_size=12,\n            learning_rate=1e-5\n        )\n        sample_texts = [\n            \"This is a test message\",\n            \"Another example text\"\n        ]\n        \n        detailed_results = classifier.predict_with_analysis(\n            sample_texts, \n            './advanced_binary_model'\n        )\n        \n        print(\"\\n🔮 DETAILED PREDICTIONS:\")\n        for result in detailed_results:\n            print(f\"Text: {result['text']}\")\n            print(f\"Prediction: {result['prediction']} ({result['confidence_level']} confidence)\")\n            print(f\"Confidence: {result['confidence']:.1%}\")\n            print(f\"Probabilities: Not Allowed={result['prob_not_allowed']:.1%}, Allowed={result['prob_allowed']:.1%}\")\n            print(\"-\" * 50)\n        \n    except Exception as e:\n        print(f\"❌ Error: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-08T18:28:52.312947Z","iopub.execute_input":"2025-09-08T18:28:52.313802Z","iopub.status.idle":"2025-09-08T19:43:09.502148Z","shell.execute_reply.started":"2025-09-08T18:28:52.313765Z","shell.execute_reply":"2025-09-08T19:43:09.501493Z"}},"outputs":[{"name":"stderr","text":"2025-09-08 18:29:03.171436: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757356143.378573      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757356143.436575      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"============================================================\n🚀 ADVANCED BINARY TEXT CLASSIFIER\n============================================================\nPyTorch: 2.6.0+cu124\nCUDA: True\nGPU: Tesla P100-PCIE-16GB\n✅ GPU: Tesla P100-PCIE-16GB\n✅ Memory: 17.1 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ef93c43e7544a2fa2075419c5133dcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9cbe7f85b5f45b29102e94c70f477d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac30f9e9905e441aab0187032660c98b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2ad06fa23cc490fbf0b11575aa0f7f5"}},"metadata":{}},{"name":"stdout","text":"📂 Loading: /kaggle/input/finaldatamodel/finalDataModel.csv\n✅ Loaded with utf-8\n📊 Shape: (19388, 2)\n📂 Columns: ['text', 'label']\n🔄 Preparing data...\n📊 Label distribution: {0: 10173, 1: 9215}\n⚖️ Class weights: [0.9529146 1.0519805]\n📊 Training: 16479 | Validation: 2909\n🚀 Advanced Training Started...\n📊 Training: 16479 | Validation: 2909\n🔄 Creating dataset with 16479 samples...\n🔄 Creating dataset with 2909 samples...\n🔄 Loading enhanced model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc219eb48d0d4c5289bdb9032e842fab"}},"metadata":{}},{"name":"stdout","text":"✅ Model on: cuda:0\n🚀 Training with advanced techniques...\n{'loss': 0.7015, 'grad_norm': 2.4555821418762207, 'learning_rate': 6.976744186046513e-07, 'epoch': 0.036390101892285295}\n{'loss': 0.7019, 'grad_norm': 1.9594006538391113, 'learning_rate': 1.424418604651163e-06, 'epoch': 0.07278020378457059}\n{'loss': 0.6989, 'grad_norm': 3.6748130321502686, 'learning_rate': 2.1511627906976745e-06, 'epoch': 0.1091703056768559}\n{'loss': 0.6893, 'grad_norm': 2.1100189685821533, 'learning_rate': 2.8779069767441865e-06, 'epoch': 0.14556040756914118}\n{'loss': 0.6644, 'grad_norm': 3.3923587799072266, 'learning_rate': 3.6046511627906977e-06, 'epoch': 0.1819505094614265}\n{'loss': 0.597, 'grad_norm': 3.772350788116455, 'learning_rate': 4.302325581395349e-06, 'epoch': 0.2183406113537118}\n{'loss': 0.4882, 'grad_norm': 3.699237108230591, 'learning_rate': 5e-06, 'epoch': 0.2547307132459971}\n{'loss': 0.4123, 'grad_norm': 4.390717506408691, 'learning_rate': 5.697674418604652e-06, 'epoch': 0.29112081513828236}\n{'loss': 0.3335, 'grad_norm': 20.70409393310547, 'learning_rate': 6.424418604651163e-06, 'epoch': 0.32751091703056767}\n{'loss': 0.3366, 'grad_norm': 14.702903747558594, 'learning_rate': 7.151162790697676e-06, 'epoch': 0.363901018922853}\n{'loss': 0.3078, 'grad_norm': 6.789659023284912, 'learning_rate': 7.848837209302325e-06, 'epoch': 0.4002911208151383}\n{'loss': 0.2697, 'grad_norm': 10.7738037109375, 'learning_rate': 8.575581395348838e-06, 'epoch': 0.4366812227074236}\n{'loss': 0.3076, 'grad_norm': 3.4230265617370605, 'learning_rate': 9.30232558139535e-06, 'epoch': 0.47307132459970885}\n{'loss': 0.2758, 'grad_norm': 5.218480587005615, 'learning_rate': 9.996764801035264e-06, 'epoch': 0.5094614264919942}\n{'loss': 0.2661, 'grad_norm': 3.2643237113952637, 'learning_rate': 9.915884826916856e-06, 'epoch': 0.5458515283842795}\n{'loss': 0.3083, 'grad_norm': 4.0270490646362305, 'learning_rate': 9.835004852798447e-06, 'epoch': 0.5822416302765647}\n{'loss': 0.199, 'grad_norm': 9.405484199523926, 'learning_rate': 9.754124878680039e-06, 'epoch': 0.61863173216885}\n{'loss': 0.2671, 'grad_norm': 9.57681655883789, 'learning_rate': 9.673244904561632e-06, 'epoch': 0.6550218340611353}\n{'loss': 0.2488, 'grad_norm': 12.4237642288208, 'learning_rate': 9.592364930443224e-06, 'epoch': 0.6914119359534207}\n{'loss': 0.2385, 'grad_norm': 5.138765335083008, 'learning_rate': 9.511484956324814e-06, 'epoch': 0.727802037845706}\n{'loss': 0.2832, 'grad_norm': 5.053461074829102, 'learning_rate': 9.430604982206405e-06, 'epoch': 0.7641921397379913}\n{'loss': 0.2486, 'grad_norm': 27.736156463623047, 'learning_rate': 9.349725008087999e-06, 'epoch': 0.8005822416302766}\n{'loss': 0.194, 'grad_norm': 13.062044143676758, 'learning_rate': 9.26884503396959e-06, 'epoch': 0.8369723435225619}\n{'loss': 0.2238, 'grad_norm': 10.846714973449707, 'learning_rate': 9.187965059851182e-06, 'epoch': 0.8733624454148472}\n{'loss': 0.1919, 'grad_norm': 8.17212963104248, 'learning_rate': 9.107085085732774e-06, 'epoch': 0.9097525473071325}\n{'loss': 0.2492, 'grad_norm': 10.283227920532227, 'learning_rate': 9.026205111614365e-06, 'epoch': 0.9461426491994177}\n{'loss': 0.2242, 'grad_norm': 12.654595375061035, 'learning_rate': 8.945325137495957e-06, 'epoch': 0.982532751091703}\n{'eval_loss': 0.2203933596611023, 'eval_accuracy': 0.9102784462014438, 'eval_balanced_accuracy': 0.9137369708376097, 'eval_f1': 0.9125041904123364, 'eval_precision': 0.850625, 'eval_recall': 0.9840925524222705, 'eval_specificity': 0.8433813892529489, 'eval_avg_confidence': 0.9487148523330688, 'eval_true_positives': 1361, 'eval_true_negatives': 1287, 'eval_false_positives': 239, 'eval_false_negatives': 22, 'eval_runtime': 34.1846, 'eval_samples_per_second': 85.097, 'eval_steps_per_second': 7.108, 'epoch': 1.0}\n{'loss': 0.2267, 'grad_norm': 12.298613548278809, 'learning_rate': 8.864445163377549e-06, 'epoch': 1.0189228529839884}\n{'loss': 0.2321, 'grad_norm': 4.740841388702393, 'learning_rate': 8.78356518925914e-06, 'epoch': 1.0553129548762736}\n{'loss': 0.2153, 'grad_norm': 4.35075044631958, 'learning_rate': 8.702685215140732e-06, 'epoch': 1.091703056768559}\n{'loss': 0.1839, 'grad_norm': 10.871844291687012, 'learning_rate': 8.621805241022324e-06, 'epoch': 1.1280931586608443}\n{'loss': 0.2093, 'grad_norm': 4.012646198272705, 'learning_rate': 8.540925266903915e-06, 'epoch': 1.1644832605531295}\n{'loss': 0.2034, 'grad_norm': 2.910125494003296, 'learning_rate': 8.460045292785507e-06, 'epoch': 1.2008733624454149}\n{'loss': 0.22, 'grad_norm': 5.993630409240723, 'learning_rate': 8.379165318667099e-06, 'epoch': 1.2372634643377}\n{'loss': 0.2139, 'grad_norm': 29.750337600708008, 'learning_rate': 8.29828534454869e-06, 'epoch': 1.2736535662299855}\n{'loss': 0.2028, 'grad_norm': inf, 'learning_rate': 8.217405370430282e-06, 'epoch': 1.3100436681222707}\n{'loss': 0.2352, 'grad_norm': 9.266366004943848, 'learning_rate': 8.13976059527661e-06, 'epoch': 1.346433770014556}\n{'loss': 0.1808, 'grad_norm': 1.245665431022644, 'learning_rate': 8.058880621158202e-06, 'epoch': 1.3828238719068413}\n{'loss': 0.2198, 'grad_norm': 10.54453182220459, 'learning_rate': 7.978000647039794e-06, 'epoch': 1.4192139737991267}\n{'loss': 0.1932, 'grad_norm': 7.944699764251709, 'learning_rate': 7.897120672921385e-06, 'epoch': 1.455604075691412}\n{'loss': 0.2036, 'grad_norm': 4.90093469619751, 'learning_rate': 7.816240698802977e-06, 'epoch': 1.4919941775836971}\n{'loss': 0.2019, 'grad_norm': 9.377458572387695, 'learning_rate': 7.735360724684569e-06, 'epoch': 1.5283842794759825}\n{'loss': 0.2031, 'grad_norm': 5.011427879333496, 'learning_rate': 7.65448075056616e-06, 'epoch': 1.564774381368268}\n{'loss': 0.1979, 'grad_norm': 2.1676249504089355, 'learning_rate': 7.573600776447753e-06, 'epoch': 1.6011644832605532}\n{'loss': 0.1948, 'grad_norm': 2.0106406211853027, 'learning_rate': 7.492720802329344e-06, 'epoch': 1.6375545851528384}\n{'loss': 0.1544, 'grad_norm': 5.598520755767822, 'learning_rate': 7.411840828210935e-06, 'epoch': 1.6739446870451238}\n{'loss': 0.1934, 'grad_norm': 9.579601287841797, 'learning_rate': 7.330960854092527e-06, 'epoch': 1.710334788937409}\n{'loss': 0.2064, 'grad_norm': 6.713501453399658, 'learning_rate': 7.250080879974118e-06, 'epoch': 1.7467248908296944}\n{'loss': 0.2011, 'grad_norm': 4.5339508056640625, 'learning_rate': 7.169200905855711e-06, 'epoch': 1.7831149927219796}\n{'loss': 0.2163, 'grad_norm': 15.294929504394531, 'learning_rate': 7.0883209317373026e-06, 'epoch': 1.8195050946142648}\n{'loss': 0.2373, 'grad_norm': 9.551889419555664, 'learning_rate': 7.007440957618894e-06, 'epoch': 1.8558951965065502}\n{'loss': 0.2055, 'grad_norm': 5.163639068603516, 'learning_rate': 6.926560983500486e-06, 'epoch': 1.8922852983988356}\n{'loss': 0.1852, 'grad_norm': 5.378941535949707, 'learning_rate': 6.845681009382078e-06, 'epoch': 1.9286754002911208}\n{'loss': 0.1673, 'grad_norm': 2.6523478031158447, 'learning_rate': 6.764801035263669e-06, 'epoch': 1.965065502183406}\n{'eval_loss': 0.22267073392868042, 'eval_accuracy': 0.915778618081815, 'eval_balanced_accuracy': 0.9175903998089514, 'eval_f1': 0.9150779896013865, 'eval_precision': 0.8788282290279628, 'eval_recall': 0.9544468546637744, 'eval_specificity': 0.8807339449541285, 'eval_avg_confidence': 0.9562146663665771, 'eval_true_positives': 1320, 'eval_true_negatives': 1344, 'eval_false_positives': 182, 'eval_false_negatives': 63, 'eval_runtime': 34.1344, 'eval_samples_per_second': 85.222, 'eval_steps_per_second': 7.119, 'epoch': 2.0}\n{'loss': 0.1974, 'grad_norm': 1.6088260412216187, 'learning_rate': 6.683921061145261e-06, 'epoch': 2.0014556040756912}\n{'loss': 0.1786, 'grad_norm': 12.947032928466797, 'learning_rate': 6.6030410870268524e-06, 'epoch': 2.037845705967977}\n{'loss': 0.1997, 'grad_norm': 4.3830389976501465, 'learning_rate': 6.522161112908444e-06, 'epoch': 2.074235807860262}\n{'loss': 0.1687, 'grad_norm': 6.252872943878174, 'learning_rate': 6.4412811387900366e-06, 'epoch': 2.1106259097525473}\n{'loss': 0.1552, 'grad_norm': 3.175144672393799, 'learning_rate': 6.360401164671628e-06, 'epoch': 2.1470160116448325}\n{'loss': 0.1678, 'grad_norm': 10.604925155639648, 'learning_rate': 6.279521190553219e-06, 'epoch': 2.183406113537118}\n{'loss': 0.1634, 'grad_norm': 2.490968942642212, 'learning_rate': 6.198641216434811e-06, 'epoch': 2.2197962154294033}\n{'loss': 0.191, 'grad_norm': 5.732156753540039, 'learning_rate': 6.117761242316403e-06, 'epoch': 2.2561863173216885}\n{'loss': 0.1952, 'grad_norm': 3.0609426498413086, 'learning_rate': 6.036881268197995e-06, 'epoch': 2.2925764192139737}\n{'loss': 0.1586, 'grad_norm': 5.503853797912598, 'learning_rate': 5.9560012940795864e-06, 'epoch': 2.328966521106259}\n{'loss': 0.1764, 'grad_norm': 4.141294956207275, 'learning_rate': 5.875121319961178e-06, 'epoch': 2.3653566229985445}\n{'loss': 0.1666, 'grad_norm': 4.9181108474731445, 'learning_rate': 5.79424134584277e-06, 'epoch': 2.4017467248908297}\n{'loss': 0.1956, 'grad_norm': 2.812595844268799, 'learning_rate': 5.713361371724362e-06, 'epoch': 2.438136826783115}\n{'loss': 0.1768, 'grad_norm': 2.1396329402923584, 'learning_rate': 5.632481397605953e-06, 'epoch': 2.4745269286754}\n{'loss': 0.1843, 'grad_norm': 8.458782196044922, 'learning_rate': 5.551601423487545e-06, 'epoch': 2.5109170305676853}\n{'loss': 0.1835, 'grad_norm': 2.995380401611328, 'learning_rate': 5.470721449369136e-06, 'epoch': 2.547307132459971}\n{'loss': 0.1633, 'grad_norm': 15.634546279907227, 'learning_rate': 5.389841475250729e-06, 'epoch': 2.583697234352256}\n{'loss': 0.1674, 'grad_norm': 10.691122055053711, 'learning_rate': 5.3089615011323204e-06, 'epoch': 2.6200873362445414}\n{'loss': 0.1624, 'grad_norm': 11.765155792236328, 'learning_rate': 5.228081527013912e-06, 'epoch': 2.656477438136827}\n{'loss': 0.1813, 'grad_norm': 3.4711573123931885, 'learning_rate': 5.147201552895503e-06, 'epoch': 2.692867540029112}\n{'loss': 0.1844, 'grad_norm': 6.666529655456543, 'learning_rate': 5.0663215787770945e-06, 'epoch': 2.7292576419213974}\n{'loss': 0.139, 'grad_norm': 17.716472625732422, 'learning_rate': 4.985441604658687e-06, 'epoch': 2.7656477438136826}\n{'loss': 0.1674, 'grad_norm': 9.454410552978516, 'learning_rate': 4.904561630540279e-06, 'epoch': 2.802037845705968}\n{'loss': 0.1617, 'grad_norm': 5.225288391113281, 'learning_rate': 4.82368165642187e-06, 'epoch': 2.8384279475982535}\n{'loss': 0.1664, 'grad_norm': 7.112277984619141, 'learning_rate': 4.742801682303462e-06, 'epoch': 2.8748180494905387}\n{'loss': 0.2031, 'grad_norm': 10.09455394744873, 'learning_rate': 4.661921708185054e-06, 'epoch': 2.911208151382824}\n{'loss': 0.2028, 'grad_norm': 9.519180297851562, 'learning_rate': 4.581041734066645e-06, 'epoch': 2.947598253275109}\n{'loss': 0.177, 'grad_norm': 5.673459529876709, 'learning_rate': 4.500161759948237e-06, 'epoch': 2.9839883551673942}\n{'eval_loss': 0.21926142275333405, 'eval_accuracy': 0.9147473358542454, 'eval_balanced_accuracy': 0.9160314964808587, 'eval_f1': 0.913104414856342, 'eval_precision': 0.8857919782460911, 'eval_recall': 0.9421547360809833, 'eval_specificity': 0.8899082568807339, 'eval_avg_confidence': 0.9499607682228088, 'eval_true_positives': 1303, 'eval_true_negatives': 1358, 'eval_false_positives': 168, 'eval_false_negatives': 80, 'eval_runtime': 34.2095, 'eval_samples_per_second': 85.035, 'eval_steps_per_second': 7.103, 'epoch': 3.0}\n{'loss': 0.1546, 'grad_norm': 2.5627760887145996, 'learning_rate': 4.4192817858298285e-06, 'epoch': 3.02037845705968}\n{'loss': 0.1523, 'grad_norm': 1.6819751262664795, 'learning_rate': 4.338401811711421e-06, 'epoch': 3.056768558951965}\n{'loss': 0.1757, 'grad_norm': 16.635051727294922, 'learning_rate': 4.257521837593012e-06, 'epoch': 3.0931586608442503}\n{'loss': 0.1343, 'grad_norm': 6.666365623474121, 'learning_rate': 4.176641863474604e-06, 'epoch': 3.1295487627365355}\n{'loss': 0.1851, 'grad_norm': 12.875922203063965, 'learning_rate': 4.095761889356196e-06, 'epoch': 3.165938864628821}\n{'loss': 0.1504, 'grad_norm': 9.35862922668457, 'learning_rate': 4.014881915237788e-06, 'epoch': 3.2023289665211063}\n{'loss': 0.1442, 'grad_norm': 1.9126312732696533, 'learning_rate': 3.934001941119379e-06, 'epoch': 3.2387190684133915}\n{'loss': 0.1486, 'grad_norm': 6.419471263885498, 'learning_rate': 3.853121967000971e-06, 'epoch': 3.2751091703056767}\n{'loss': 0.1368, 'grad_norm': 10.105688095092773, 'learning_rate': 3.7722419928825625e-06, 'epoch': 3.3114992721979624}\n{'loss': 0.1344, 'grad_norm': 9.373411178588867, 'learning_rate': 3.691362018764154e-06, 'epoch': 3.3478893740902476}\n{'loss': 0.1264, 'grad_norm': 8.54293155670166, 'learning_rate': 3.6104820446457462e-06, 'epoch': 3.3842794759825328}\n{'loss': 0.1712, 'grad_norm': 7.227280139923096, 'learning_rate': 3.5296020705273375e-06, 'epoch': 3.420669577874818}\n{'loss': 0.1395, 'grad_norm': 4.27556037902832, 'learning_rate': 3.4487220964089295e-06, 'epoch': 3.457059679767103}\n{'loss': 0.15, 'grad_norm': 0.3433271646499634, 'learning_rate': 3.367842122290521e-06, 'epoch': 3.493449781659389}\n{'loss': 0.1481, 'grad_norm': 3.7859292030334473, 'learning_rate': 3.2869621481721132e-06, 'epoch': 3.529839883551674}\n{'loss': 0.1552, 'grad_norm': 0.9412720203399658, 'learning_rate': 3.2060821740537045e-06, 'epoch': 3.566229985443959}\n{'loss': 0.1261, 'grad_norm': 4.63873291015625, 'learning_rate': 3.125202199935296e-06, 'epoch': 3.6026200873362444}\n{'loss': 0.1548, 'grad_norm': 5.328318119049072, 'learning_rate': 3.044322225816888e-06, 'epoch': 3.6390101892285296}\n{'loss': 0.1449, 'grad_norm': 14.4724702835083, 'learning_rate': 2.9634422516984794e-06, 'epoch': 3.6754002911208152}\n{'loss': 0.1727, 'grad_norm': 3.0015344619750977, 'learning_rate': 2.8825622775800715e-06, 'epoch': 3.7117903930131004}\n{'loss': 0.1351, 'grad_norm': 3.6571438312530518, 'learning_rate': 2.801682303461663e-06, 'epoch': 3.7481804949053856}\n{'loss': 0.1453, 'grad_norm': 24.97626304626465, 'learning_rate': 2.720802329343255e-06, 'epoch': 3.7845705967976713}\n{'loss': 0.1534, 'grad_norm': 9.247597694396973, 'learning_rate': 2.6399223552248464e-06, 'epoch': 3.8209606986899565}\n{'loss': 0.141, 'grad_norm': 6.310153007507324, 'learning_rate': 2.559042381106438e-06, 'epoch': 3.8573508005822417}\n{'loss': 0.1571, 'grad_norm': 20.67334747314453, 'learning_rate': 2.47816240698803e-06, 'epoch': 3.893740902474527}\n{'loss': 0.1459, 'grad_norm': 10.842249870300293, 'learning_rate': 2.3972824328696217e-06, 'epoch': 3.930131004366812}\n{'loss': 0.157, 'grad_norm': 18.56820297241211, 'learning_rate': 2.3164024587512134e-06, 'epoch': 3.9665211062590977}\n{'eval_loss': 0.25856226682662964, 'eval_accuracy': 0.9006531454107941, 'eval_balanced_accuracy': 0.899751854810662, 'eval_f1': 0.894022735606894, 'eval_precision': 0.9069940476190477, 'eval_recall': 0.8814172089660159, 'eval_specificity': 0.918086500655308, 'eval_avg_confidence': 0.953715980052948, 'eval_true_positives': 1219, 'eval_true_negatives': 1401, 'eval_false_positives': 125, 'eval_false_negatives': 164, 'eval_runtime': 34.122, 'eval_samples_per_second': 85.253, 'eval_steps_per_second': 7.121, 'epoch': 4.0}\n{'loss': 0.176, 'grad_norm': 6.674283027648926, 'learning_rate': 2.235522484632805e-06, 'epoch': 4.0029112081513825}\n{'loss': 0.1329, 'grad_norm': 4.573486328125, 'learning_rate': 2.1546425105143967e-06, 'epoch': 4.039301310043668}\n{'loss': 0.1243, 'grad_norm': 12.180615425109863, 'learning_rate': 2.0737625363959883e-06, 'epoch': 4.075691411935954}\n{'loss': 0.1093, 'grad_norm': 5.144412040710449, 'learning_rate': 1.9928825622775804e-06, 'epoch': 4.1120815138282385}\n{'loss': 0.1229, 'grad_norm': 12.242866516113281, 'learning_rate': 1.912002588159172e-06, 'epoch': 4.148471615720524}\n{'loss': 0.1395, 'grad_norm': 4.873035430908203, 'learning_rate': 1.8311226140407637e-06, 'epoch': 4.184861717612809}\n{'loss': 0.1521, 'grad_norm': 2.6884360313415527, 'learning_rate': 1.7502426399223555e-06, 'epoch': 4.2212518195050945}\n{'loss': 0.1268, 'grad_norm': 5.5826849937438965, 'learning_rate': 1.6693626658039472e-06, 'epoch': 4.25764192139738}\n{'loss': 0.1754, 'grad_norm': 11.980247497558594, 'learning_rate': 1.5884826916855388e-06, 'epoch': 4.294032023289665}\n{'loss': 0.1284, 'grad_norm': 4.267226219177246, 'learning_rate': 1.5076027175671305e-06, 'epoch': 4.330422125181951}\n{'loss': 0.1133, 'grad_norm': 12.213031768798828, 'learning_rate': 1.4267227434487221e-06, 'epoch': 4.366812227074236}\n{'loss': 0.1478, 'grad_norm': 13.164663314819336, 'learning_rate': 1.345842769330314e-06, 'epoch': 4.403202328966521}\n{'loss': 0.163, 'grad_norm': 26.21306610107422, 'learning_rate': 1.2649627952119056e-06, 'epoch': 4.439592430858807}\n{'loss': 0.1262, 'grad_norm': 5.132431507110596, 'learning_rate': 1.1840828210934975e-06, 'epoch': 4.475982532751091}\n{'loss': 0.1244, 'grad_norm': 11.115962028503418, 'learning_rate': 1.1032028469750891e-06, 'epoch': 4.512372634643377}\n{'loss': 0.1246, 'grad_norm': 2.8898041248321533, 'learning_rate': 1.0223228728566808e-06, 'epoch': 4.548762736535663}\n{'loss': 0.1437, 'grad_norm': 32.64805603027344, 'learning_rate': 9.446780977030089e-07, 'epoch': 4.585152838427947}\n{'loss': 0.1277, 'grad_norm': 13.784189224243164, 'learning_rate': 8.637981235846005e-07, 'epoch': 4.621542940320233}\n{'loss': 0.0902, 'grad_norm': 11.438926696777344, 'learning_rate': 7.829181494661923e-07, 'epoch': 4.657933042212518}\n{'loss': 0.1214, 'grad_norm': 11.044349670410156, 'learning_rate': 7.02038175347784e-07, 'epoch': 4.6943231441048034}\n{'loss': 0.0887, 'grad_norm': 15.600713729858398, 'learning_rate': 6.211582012293756e-07, 'epoch': 4.730713245997089}\n{'loss': 0.1274, 'grad_norm': 13.682021141052246, 'learning_rate': 5.402782271109674e-07, 'epoch': 4.767103347889374}\n{'loss': 0.1605, 'grad_norm': 18.046586990356445, 'learning_rate': 4.5939825299255904e-07, 'epoch': 4.8034934497816595}\n{'loss': 0.1475, 'grad_norm': 29.671602249145508, 'learning_rate': 3.785182788741508e-07, 'epoch': 4.839883551673944}\n{'loss': 0.1679, 'grad_norm': 13.615656852722168, 'learning_rate': 2.976383047557425e-07, 'epoch': 4.87627365356623}\n{'loss': 0.1474, 'grad_norm': 13.691044807434082, 'learning_rate': 2.167583306373342e-07, 'epoch': 4.9126637554585155}\n{'loss': 0.1007, 'grad_norm': 0.7050759196281433, 'learning_rate': 1.3911355548366226e-07, 'epoch': 4.9490538573508}\n{'loss': 0.1009, 'grad_norm': 1.3314968347549438, 'learning_rate': 5.8233581365253973e-08, 'epoch': 4.985443959243086}\n{'eval_loss': 0.28413259983062744, 'eval_accuracy': 0.9047782743210725, 'eval_balanced_accuracy': 0.9047000698426597, 'eval_f1': 0.9001801801801802, 'eval_precision': 0.8972701149425287, 'eval_recall': 0.9031091829356471, 'eval_specificity': 0.9062909567496723, 'eval_avg_confidence': 0.9657771587371826, 'eval_true_positives': 1249, 'eval_true_negatives': 1383, 'eval_false_positives': 143, 'eval_false_negatives': 134, 'eval_runtime': 34.1759, 'eval_samples_per_second': 85.119, 'eval_steps_per_second': 7.11, 'epoch': 5.0}\n{'train_runtime': 4375.2636, 'train_samples_per_second': 18.832, 'train_steps_per_second': 0.785, 'train_loss': 0.20517486599334983, 'epoch': 5.0}\n💾 Saving to ./advanced_binary_model\n📊 Final evaluation...\n{'eval_loss': 0.22267073392868042, 'eval_accuracy': 0.915778618081815, 'eval_balanced_accuracy': 0.9175903998089514, 'eval_f1': 0.9150779896013865, 'eval_precision': 0.8788282290279628, 'eval_recall': 0.9544468546637744, 'eval_specificity': 0.8807339449541285, 'eval_avg_confidence': 0.9562146663665771, 'eval_true_positives': 1320, 'eval_true_negatives': 1344, 'eval_false_positives': 182, 'eval_false_negatives': 63, 'eval_runtime': 34.1767, 'eval_samples_per_second': 85.117, 'eval_steps_per_second': 7.11, 'epoch': 5.0}\n\n============================================================\n🏆 ADVANCED TRAINING RESULTS:\neval_loss: 0.2227\neval_accuracy: 0.9158\neval_balanced_accuracy: 0.9176\neval_f1: 0.9151\neval_precision: 0.8788\neval_recall: 0.9544\neval_specificity: 0.8807\neval_avg_confidence: 95.6%\neval_true_positives: 1320\neval_true_negatives: 1344\neval_false_positives: 182\neval_false_negatives: 63\neval_runtime: 34.1767\neval_samples_per_second: 85.1170\neval_steps_per_second: 7.1100\nepoch: 5.0000\n============================================================\n🧪 Testing with known samples...\n🔄 Loading model for enhanced prediction...\n🔮 Analyzing 8 samples...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6538b82d0ca54e5f80a972c0414430b3"}},"metadata":{}},{"name":"stdout","text":"\n📊 PREDICTION ANALYSIS:\nAverage confidence: 99.1%\nHigh confidence predictions: 8/8 (100.0%)\n\n✅ ALLOWED SAMPLES:\n✅ allowed (99.3%): Hello, how are you today?\n✅ allowed (98.9%): What's the weather like?\n✅ allowed (99.6%): Thank you for your help\n✅ allowed (99.1%): Good morning everyone\n✅ allowed (99.1%): I hope you have a great day\n✅ allowed (98.8%): This is a normal conversation\n✅ allowed (99.3%): Can you help me with this question?\n✅ allowed (98.8%): That's very interesting, tell me more\n🔄 Loading model for enhanced prediction...\n🔮 Analyzing 4 samples...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95fb7baba0d748ab8ff32ab5126a234a"}},"metadata":{}},{"name":"stdout","text":"\n📊 PREDICTION ANALYSIS:\nAverage confidence: 97.5%\nHigh confidence predictions: 4/4 (100.0%)\n\n❌ NOT ALLOWED SAMPLES:\n❌ allowed (96.7%): This contains inappropriate content\n❌ allowed (98.7%): Offensive language example\n❌ allowed (97.2%): Hate speech sample\n❌ allowed (97.3%): Violent content description\n🔄 Loading model for enhanced prediction...\n🔮 Analyzing 2 samples...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5243635df80442e78a31b0da7a3ae57c"}},"metadata":{}},{"name":"stdout","text":"\n📊 PREDICTION ANALYSIS:\nAverage confidence: 98.8%\nHigh confidence predictions: 2/2 (100.0%)\n\n🔮 DETAILED PREDICTIONS:\nText: This is a test message\nPrediction: allowed (HIGH confidence)\nConfidence: 98.6%\nProbabilities: Not Allowed=1.4%, Allowed=98.6%\n--------------------------------------------------\nText: Another example text\nPrediction: allowed (HIGH confidence)\nConfidence: 98.9%\nProbabilities: Not Allowed=1.1%, Allowed=98.9%\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Quick test - run this in your notebook:\ntry:\n    from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n    import torch\n    \n    # Load your model\n    model = XLMRobertaForSequenceClassification.from_pretrained('./advanced_binary_model')\n    tokenizer = XLMRobertaTokenizer.from_pretrained('./advanced_binary_model')\n    \n    # Test it\n    test_text =\"Genocide in gaza increases day after day\"\n    inputs = tokenizer(test_text, return_tensors='pt', padding=True, truncation=True)\n    \n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n        pred = torch.argmax(outputs.logits, dim=-1).item()\n        conf = torch.softmax(outputs.logits, dim=-1).max().item()\n    \n    result = \"allowed\" if pred == 1 else \"not allowed\"\n    print(f\"✅ SUCCESS! Model works: '{result}' (confidence: {conf:.1%})\")\n    \nexcept Exception as e:\n    print(f\"❌ Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T20:16:33.784265Z","iopub.execute_input":"2025-09-08T20:16:33.784976Z","iopub.status.idle":"2025-09-08T20:16:35.218240Z","shell.execute_reply.started":"2025-09-08T20:16:33.784953Z","shell.execute_reply":"2025-09-08T20:16:35.217484Z"}},"outputs":[{"name":"stdout","text":"✅ SUCCESS! Model works: 'allowed' (confidence: 93.4%)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"try:\n    from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n    import torch\n\n    # Function to load a model and test a text\n    def test_model(model_path, test_text):\n        # Load model + tokenizer\n        model = XLMRobertaForSequenceClassification.from_pretrained(model_path)\n        tokenizer = XLMRobertaTokenizer.from_pretrained(model_path)\n\n        # Prepare inputs\n        inputs = tokenizer(test_text, return_tensors='pt', padding=True, truncation=True)\n\n        # Run inference\n        model.eval()\n        with torch.no_grad():\n            outputs = model(**inputs)\n            pred = torch.argmax(outputs.logits, dim=-1).item()\n            conf = torch.softmax(outputs.logits, dim=-1).max().item()\n\n        result = \"allowed\" if pred == 1 else \"not allowed\"\n        return {\"model\": model_path, \"prediction\": result, \"confidence\": conf}\n\n    # 🔎 Your test text\n    test_text = \"\"\n\n    # Two models to compare\n    model1_path = \"./advanced_binary_model\"\n    model2_path = \"/kaggle/input/model3/transformers/default/1/ainexus_model\"   # ⚠️ change this to your second model folder\n\n    # Run both models\n    result1 = test_model(model1_path, test_text)\n    result2 = test_model(model2_path, test_text)\n\n    # Display comparison\n    print(\"🔮 COMPARISON RESULTS:\")\n    for r in [result1, result2]:\n        print(f\"Model: {r['model']}\")\n        print(f\"Prediction: {r['prediction']} (confidence: {r['confidence']:.1%})\")\n        print(\"-\" * 50)\n\nexcept Exception as e:\n    print(f\"❌ Error: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T21:26:34.913351Z","iopub.execute_input":"2025-09-08T21:26:34.914079Z","iopub.status.idle":"2025-09-08T21:26:37.766683Z","shell.execute_reply.started":"2025-09-08T21:26:34.914052Z","shell.execute_reply":"2025-09-08T21:26:37.766022Z"}},"outputs":[{"name":"stdout","text":"🔮 COMPARISON RESULTS:\nModel: ./advanced_binary_model\nPrediction: allowed (confidence: 95.9%)\n--------------------------------------------------\nModel: /kaggle/input/model3/transformers/default/1/ainexus_model\nPrediction: allowed (confidence: 89.7%)\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":32}]}